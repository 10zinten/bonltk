{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "from pathlib import Path\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fn = Path('../input/tokenized_paragraphs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_paras = [para.split(' ') for para in data_fn.read_text().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ན་མོ་',\n",
       " 'གུ་རུ་',\n",
       " 'དེ་བ་',\n",
       " 'ཌཱ་ཀི་',\n",
       " 'ནཱི་',\n",
       " 'ཡཻ',\n",
       " '།_',\n",
       " 'དགོངས་པ',\n",
       " 'འི་',\n",
       " 'སྟོབས་',\n",
       " 'དང་',\n",
       " 'ཚུལ་ལྡན་',\n",
       " 'ཆོ་ག',\n",
       " 'འི་',\n",
       " 'མཐུ',\n",
       " 'ས',\n",
       " '།_།',\n",
       " 'ཐོག་མེད་',\n",
       " 'འཁྲུལ་',\n",
       " 'པ',\n",
       " 'འི་',\n",
       " 'འཆིང་བ་',\n",
       " 'སྐད་ཅིག་',\n",
       " 'ལ',\n",
       " '།_།',\n",
       " 'བྲལ་',\n",
       " 'ན',\n",
       " 'ས་',\n",
       " 'མངོན་སུམ་',\n",
       " 'ཡེ་ཤེས་',\n",
       " 'སད་',\n",
       " 'མཛད་པ',\n",
       " '།_།',\n",
       " 'དཀྱིལ་འཁོར་',\n",
       " 'དབང་ཕྱུག་',\n",
       " 'དཔལ་ལྡན་',\n",
       " 'བླ་མ',\n",
       " 'ར་',\n",
       " 'འདུད',\n",
       " '།_།',\n",
       " 'རྡོ་རྗེ་',\n",
       " 'ཐེག་པ',\n",
       " 'འི་',\n",
       " 'རྩ་བ་',\n",
       " 'སྨིན་',\n",
       " 'བྱེད་',\n",
       " 'ཀྱི',\n",
       " '།_།',\n",
       " 'ཚུལ་',\n",
       " 'འདི་',\n",
       " 'ཟབ་',\n",
       " 'རྒྱ',\n",
       " 'ས་',\n",
       " 'ཉིད་',\n",
       " 'ཕྱི',\n",
       " 'ར་',\n",
       " 'རྟོགས་དཀའ་',\n",
       " 'ཡང་',\n",
       " '།_།',\n",
       " 'དང་པོ',\n",
       " 'འི་',\n",
       " 'ལས་ཅན་',\n",
       " 'ཕྱོགས་',\n",
       " 'ཙམ་',\n",
       " 'ངེས་',\n",
       " 'རྙེད་',\n",
       " 'ཕྱི',\n",
       " 'ར',\n",
       " '།_།',\n",
       " 'གོ་',\n",
       " 'བདེ',\n",
       " 'འི་',\n",
       " 'ངག་',\n",
       " 'གི',\n",
       " 'ས་',\n",
       " 'མདོར་བསྡུས་',\n",
       " 'བརྗོད་པ',\n",
       " 'ར་',\n",
       " 'བྱ',\n",
       " '།_།',\n",
       " 'དེ',\n",
       " 'འང་',\n",
       " 'རྡོ་རྗེ་',\n",
       " 'ཐེག་པ',\n",
       " 'འི་',\n",
       " 'ལམ་',\n",
       " 'གྱི་',\n",
       " 'གནད་',\n",
       " 'ཐམས་ཅད་',\n",
       " 'ཚང་',\n",
       " 'ཞིང་',\n",
       " 'ཁྱད་པར་',\n",
       " 'གསང་སྔགས་',\n",
       " 'ཀྱི་',\n",
       " 'རྒྱུད་',\n",
       " 'ལུང་',\n",
       " 'མན་ངག་',\n",
       " 'རྣམས་',\n",
       " 'ལ་',\n",
       " 'ཐོས་བསམ་',\n",
       " 'སྒོམ་པ་',\n",
       " 'གང་',\n",
       " 'བྱེད་',\n",
       " 'ཀྱང་',\n",
       " 'ངེས་པ',\n",
       " 'ར་',\n",
       " 'སྔོན་',\n",
       " 'དུ་མ་',\n",
       " 'སོང་',\n",
       " 'ཐབས་མེད་',\n",
       " 'པ་',\n",
       " 'ནི་',\n",
       " 'དབང་',\n",
       " 'བསྐུར་བ་',\n",
       " 'དང་',\n",
       " '།_',\n",
       " 'དེ་ལས་',\n",
       " 'ཐོབ་པ',\n",
       " 'འི་',\n",
       " 'དམ་ཚིག་',\n",
       " 'མི་',\n",
       " 'ཉམས་པ',\n",
       " 'ར་',\n",
       " 'སྲུང་བ་',\n",
       " 'ཁོ་ན་',\n",
       " 'ཡིན་',\n",
       " 'ཞིང་',\n",
       " '།_',\n",
       " 'དབང་མ་',\n",
       " 'ནོ',\n",
       " 'ས་',\n",
       " 'པ',\n",
       " 'ར་',\n",
       " 'གསང་སྔགས་',\n",
       " 'ལ་',\n",
       " 'ཞུགས་',\n",
       " 'ན་',\n",
       " 'དངོས་གྲུབ་',\n",
       " 'མི་',\n",
       " 'འགྲུབ་པ',\n",
       " 'ར་',\n",
       " 'མ་ཟད་',\n",
       " 'འདི་',\n",
       " 'དང་',\n",
       " 'གཏན་',\n",
       " 'ཏུ་',\n",
       " 'ཉམས་པ་',\n",
       " 'ཆེན་པོ',\n",
       " 'ར་',\n",
       " 'འགྱུར་བ',\n",
       " 'ས',\n",
       " '།']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_paras[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58888"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for para in tokenized_paras:\n",
    "    for i in para:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ས་', '།_', 'འི་', 'ར་', '།_།', 'ལ་', 'དང་', 'ན', 'ཀྱི་', 'དུ་']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why I seperate the training of the model in 3 steps:\n",
    "I prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n",
    "1. `Word2Vec()`: \n",
    ">In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n",
    "2. `.build_vocab()`: \n",
    ">Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. I noticed that these two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "3. `.train()`:\n",
    ">Finally, trains the model.<br>\n",
    "The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameters:\n",
    "\n",
    "* `min_count` <font color='purple'>=</font> <font color='green'>int</font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "\n",
    "* `window` <font color='purple'>=</font> <font color='green'>int</font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n",
    "\n",
    "\n",
    "* `size` <font color='purple'>=</font> <font color='green'>int</font> - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "\n",
    "* `sample` <font color='purple'>=</font> <font color='green'>float</font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n",
    "\n",
    "\n",
    "* `alpha` <font color='purple'>=</font> <font color='green'>float</font> - The initial learning rate - (0.01, 0.05)\n",
    "\n",
    "\n",
    "* `min_alpha` <font color='purple'>=</font> <font color='green'>float</font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "\n",
    "* `negative` <font color='purple'>=</font> <font color='green'>int</font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "\n",
    "* `workers` <font color='purple'>=</font> <font color='green'>int</font> - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension of word embedding\n",
    "The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition (Melamud et al., 2016) [3] or part-of-speech (POS) tagging (Plank et al., 2016) [4], while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis (Ruder et al., 2016) [5].\n",
    "\n",
    "- [3] -> http://arxiv.org/abs/1601.00893\n",
    "- [4] -> Plank, B., Søgaard, A., & Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. \n",
    "- [5] -> http://arxiv.org/abs/1609.02745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=5,\n",
    "                     size=150,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vocabulary Table:\n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.11 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(tokenized_paras, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model:\n",
    "_Parameters of the training:_\n",
    "* `total_examples` <font color='purple'>=</font> <font color='green'>int</font> - Count of sentences;\n",
    "* `epochs` <font color='purple'>=</font> <font color='green'>int</font> - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ce6b45daa99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_paras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time to train the model: {} mins'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(tokenized_paras, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3729301 , -0.8534347 , -1.0309534 ,  0.8542432 ,  1.2413125 ,\n",
       "        0.1571531 , -0.10291172,  0.58237725, -2.644014  , -1.8357233 ,\n",
       "        0.5710879 ,  0.83698714,  0.6882576 ,  0.8948287 , -0.02084561,\n",
       "        2.117036  ,  1.7047074 ,  2.2853692 ,  3.6181774 , -0.58283055,\n",
       "       -0.26574504,  0.07433303,  2.093018  , -0.48373508, -0.90534794,\n",
       "        0.8568248 , -2.666746  ,  0.05026199,  1.6255131 , -3.249759  ,\n",
       "        0.03756811, -0.74018884, -1.1021843 ,  0.8330723 ,  0.24384524,\n",
       "       -0.436396  ,  0.82031775,  1.571845  ,  0.66659415, -1.6746459 ,\n",
       "        0.3454264 , -0.09310634, -1.4394299 , -0.40920028,  1.1556567 ,\n",
       "        0.38380772,  0.78428346,  1.3377736 , -0.9033879 ,  0.21484745,\n",
       "        0.5911416 , -1.1893706 ,  0.5863942 , -0.39954457, -1.1904216 ,\n",
       "        2.515153  ,  0.483994  , -0.21034713, -1.2771081 ,  0.49367416,\n",
       "        0.7438269 , -1.1765704 ,  1.0646936 , -0.5540529 , -0.72172844,\n",
       "        0.08064974,  1.1871132 , -2.1466894 , -1.2714894 , -2.0902472 ,\n",
       "       -0.49627268, -2.081239  ,  3.2932491 ,  0.1840129 ,  1.3109959 ,\n",
       "       -1.4077077 ,  4.3438067 ,  0.8321069 ,  1.6677233 ,  0.05167065,\n",
       "       -0.40717888,  3.1815    , -0.9852499 , -0.83479834, -1.8297144 ,\n",
       "       -0.662315  ,  1.1932558 ,  1.4562324 ,  0.43073586, -1.2001997 ,\n",
       "        0.00835984, -1.3540074 , -3.0827186 , -1.0439548 , -1.2913095 ,\n",
       "       -1.6129723 ,  0.7797926 ,  0.7831732 , -0.5479689 ,  0.54203975,\n",
       "       -1.7527959 ,  0.42178974, -0.5645894 , -1.2086226 ,  1.2670033 ,\n",
       "        1.9524311 ,  1.8745785 , -2.333907  , -1.7566147 , -0.750876  ,\n",
       "        0.05573343, -1.9756233 ,  0.78707075, -2.1348531 , -0.11806194,\n",
       "       -1.1820506 , -1.4325802 ,  1.9121933 , -1.6680902 ,  0.14564613,\n",
       "        1.2753063 , -0.06598692,  0.71988195, -0.70861256,  2.2316113 ,\n",
       "        0.34990895, -1.4370155 , -3.2273962 ,  0.95979494, -1.3285865 ,\n",
       "        0.3163935 , -1.0856521 ,  2.2262206 ,  1.1223254 , -0.5381461 ,\n",
       "        2.2120025 , -1.5612227 ,  2.1913786 , -1.1596028 ,  2.464901  ,\n",
       "       -1.5938073 ,  1.9249473 ,  0.19897988,  1.2352128 , -2.4546347 ,\n",
       "       -1.1000104 , -1.384427  ,  0.52425265, -1.0735543 , -0.85703593],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['ཞུགས']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.save_word2vec_format(\"./bo_word2vec\",\n",
    "                              \"./vocab\",\n",
    "                               binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook_source__.ipynb  vocabulary  word2vec_org\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text = KeyedVectors.load_word2vec_format('word2vec_org', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3729301 , -0.8534347 , -1.0309534 ,  0.8542432 ,  1.2413125 ,\n",
       "        0.1571531 , -0.10291172,  0.58237725, -2.644014  , -1.8357233 ,\n",
       "        0.5710879 ,  0.83698714,  0.6882576 ,  0.8948287 , -0.02084561,\n",
       "        2.117036  ,  1.7047074 ,  2.2853692 ,  3.6181774 , -0.58283055,\n",
       "       -0.26574504,  0.07433303,  2.093018  , -0.48373508, -0.90534794,\n",
       "        0.8568248 , -2.666746  ,  0.05026199,  1.6255131 , -3.249759  ,\n",
       "        0.03756811, -0.74018884, -1.1021843 ,  0.8330723 ,  0.24384524,\n",
       "       -0.436396  ,  0.82031775,  1.571845  ,  0.66659415, -1.6746459 ,\n",
       "        0.3454264 , -0.09310634, -1.4394299 , -0.40920028,  1.1556567 ,\n",
       "        0.38380772,  0.78428346,  1.3377736 , -0.9033879 ,  0.21484745,\n",
       "        0.5911416 , -1.1893706 ,  0.5863942 , -0.39954457, -1.1904216 ,\n",
       "        2.515153  ,  0.483994  , -0.21034713, -1.2771081 ,  0.49367416,\n",
       "        0.7438269 , -1.1765704 ,  1.0646936 , -0.5540529 , -0.72172844,\n",
       "        0.08064974,  1.1871132 , -2.1466894 , -1.2714894 , -2.0902472 ,\n",
       "       -0.49627268, -2.081239  ,  3.2932491 ,  0.1840129 ,  1.3109959 ,\n",
       "       -1.4077077 ,  4.3438067 ,  0.8321069 ,  1.6677233 ,  0.05167065,\n",
       "       -0.40717888,  3.1815    , -0.9852499 , -0.83479834, -1.8297144 ,\n",
       "       -0.662315  ,  1.1932558 ,  1.4562324 ,  0.43073586, -1.2001997 ,\n",
       "        0.00835984, -1.3540074 , -3.0827186 , -1.0439548 , -1.2913095 ,\n",
       "       -1.6129723 ,  0.7797926 ,  0.7831732 , -0.5479689 ,  0.54203975,\n",
       "       -1.7527959 ,  0.42178974, -0.5645894 , -1.2086226 ,  1.2670033 ,\n",
       "        1.9524311 ,  1.8745785 , -2.333907  , -1.7566147 , -0.750876  ,\n",
       "        0.05573343, -1.9756233 ,  0.78707075, -2.1348531 , -0.11806194,\n",
       "       -1.1820506 , -1.4325802 ,  1.9121933 , -1.6680902 ,  0.14564613,\n",
       "        1.2753063 , -0.06598692,  0.71988195, -0.70861256,  2.2316113 ,\n",
       "        0.34990895, -1.4370155 , -3.2273962 ,  0.95979494, -1.3285865 ,\n",
       "        0.3163935 , -1.0856521 ,  2.2262206 ,  1.1223254 , -0.5381461 ,\n",
       "        2.2120025 , -1.5612227 ,  2.1913786 , -1.1596028 ,  2.464901  ,\n",
       "       -1.5938073 ,  1.9249473 ,  0.19897988,  1.2352128 , -2.4546347 ,\n",
       "       -1.1000104 , -1.384427  ,  0.52425265, -1.0735543 , -0.85703593],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text['ཞུགས']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ས་ 687133\r\n",
      "།_ 533771\r\n",
      "འི་ 506420\r\n",
      "ར་ 462038\r\n",
      "།_། 341664\r\n",
      "ལ་ 208912\r\n",
      "དང་ 197993\r\n",
      "ན 144106\r\n",
      "ཀྱི་ 140586\r\n",
      "དུ་ 137221\r\n"
     ]
    }
   ],
   "source": [
    "!head vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
